# -*- coding: utf-8 -*-
"""Federated_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s1tKAA0r5Dry0MGVy4IxMM7-6rQg88cV
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

def mnist_processed():
  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

  s = x_train.shape
  x_train = x_train.reshape(s[0],s[1], s[2], 1)
  x_train = x_train.astype('float32')
  x_train = (x_train -(255/2))/(255/2)
  
  s = x_test.shape
  x_test = x_test.reshape(s[0],s[1], s[2], 1)
  x_test = x_test.astype('float32')
  x_test = (x_test -(255/2))/(255/2)

  return x_train,y_train,x_test,  y_test

def model_discriminator():

	model = keras.Sequential()
	model.add(keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))
	model.add(keras.layers.LeakyReLU())
	model.add(keras.layers.Dropout(0.3))

	model.add(keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
	model.add(keras.layers.LeakyReLU())
	model.add(keras.layers.Dropout(0.3))

	model.add(keras.layers.Flatten())
	model.add(keras.layers.Dense(11))

	return model

def model_generator():
	model = keras.models.Sequential()
	# foundation for 7x7 image
	n_nodes = 128 * 7 * 7
	model.add(keras.layers.Dense(n_nodes, input_dim=100))
	model.add(keras.layers.BatchNormalization())
	model.add(keras.layers.LeakyReLU(alpha=0.2))
	model.add(keras.layers.Reshape((7, 7, 128)))
	# upsample to 14x14
	model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(keras.layers.BatchNormalization())
	model.add(keras.layers.LeakyReLU(alpha=0.2))
	# upsample to 28x28
	model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(keras.layers.BatchNormalization())
	model.add(keras.layers.LeakyReLU(alpha=0.2))
	model.add(keras.layers.Conv2D(1, (7,7), activation='tanh', padding='same'))
	return model

def gan(generator, discriminator):
	discriminator.trainable = False
	model = keras.models.Sequential()
	model.add(generator)
	model.add(discriminator)
	return model

def generate_latent(count, dim):
  points = np.random.rand(dim*count)
  points = points.reshape(count, dim)
  return points

def generator_sample(count, model, label):
  points = generate_latent(count, 100)
  x_fake = model.predict(points)
  y_fake = np.array([label]*count)
  return x_fake, y_fake

def setup():

  participant_model = {}
  participant_data = {}
  participant_label = {}
  data_train, label_train, data_test, label_test = mnist_processed()

  
  

  central_model = model_discriminator()
  central_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2, decay=1e-7),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['sparse_categorical_accuracy'])
  central_model.fit(data_train[:3000], label_train[:3000], validation_split=0, epochs=10, batch_size = 256)
  print('------------------------Central Model----------------------------------------')
  print(central_model.summary())
  print('----------------------------------------------------------------------------')
  

  
  ########################################################################################
  for i in range(10):

    participant_model[i] = model_discriminator()
    participant_model[i].compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['sparse_categorical_accuracy'])
    
    participant_data[i] = data_train[label_train == i]
    participant_label[i] = label_train[label_train == i]

  ########################################################################################
  attack_discriminator = model_discriminator()
  attack_discriminator.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['sparse_categorical_accuracy'])
  
  print('------------------------attack_discriminator----------------------------------------')
  print(attack_discriminator.summary())
  
  attack_generator = model_generator()
  attack_generator.compile()

  print('------------------------attack_generator----------------------------------------')
  print(attack_generator.summary())
  
  gan_model = gan(attack_generator, attack_discriminator)
  gan_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['sparse_categorical_accuracy'])
  
  print('------------------------gan_model----------------------------------------')
  print(gan_model.summary())

  return central_model, participant_model, participant_data, participant_label, attack_discriminator , attack_generator, data_test, label_test, gan_model

def federated_learning():
  rounds = 500
  no_of_participants = 10
  attacker_id = 5
  target = 3
  accuracy_threshold = 0.6
  acc=0
  central_model, participant_model, participant_data, participant_label, attack_discriminator , attack_generator, test, test_label, gan_model = setup()
  

  for r in range(150):
    print('------------------------Round: {}-------------------------'.format(r+1))

    downloaded_model = central_model.get_weights()
    weights = []

    for i in range(no_of_participants):

      model = participant_model[i]
      train = participant_data[i]
      label = participant_label[i]

      if (acc > accuracy_threshold) and (i == attacker_id):

        attack_discriminator.set_weights(downloaded_model)

        #inps = np.random.randint(0, label.shape[0], int(0.1*label.shape[0]))
        
        
        #ttack_discriminator.fit(train[inps], label[inps],  validation_split=0, epochs=1, shuffle= True,  batch_size = 256)
        X_gan = generate_latent(256*10, 100)
        y_gan = np.array([target]*256*10)
        gan_model.fit(X_gan, y_gan, epochs=1, batch_size = 256, verbose = 0)

        #X_tr, Y_tr = generator_sample(30*10, attack_generator, 10)
        

        #attack_discriminator.fit(X_tr, Y_tr,  validation_split=0, epochs=1, shuffle= True,  batch_size = 300)
        #model.set_weights(attack_discriminator.get_weights())

      inps = np.random.randint(0, label.shape[0], int(0.1*label.shape[0]))
      model.set_weights(downloaded_model)
      model.fit(train[inps], label[inps],  validation_split=0, epochs=1, batch_size = 500, verbose = 0)
      if i:
        weights += np.array(model.get_weights())
      else:
        weights = np.array(model.get_weights())

    weight_avg = np.true_divide(weights,no_of_participants).tolist()
    central_model.set_weights(weight_avg)
    del weights, weight_avg

    loss, acc = central_model.evaluate(test, test_label)

    if(acc>accuracy_threshold):

      X, Y = generator_sample(5, attack_generator, target)
      for i in range(5):
        plt.subplot(1, 5, 1 + i)
        plt.axis('off')
        plt.imshow(X[i, :, :, 0], cmap='gray')
      plt.show()
    if acc > 0.9:
      break

federated_learning()

